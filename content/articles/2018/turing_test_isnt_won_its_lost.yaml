title: "The Turing test isn't won by machines, it's lost by humans"
description: "Humanity has already failed the Turing test at a fundamental level, we've just passed the asymptomatic period."
image: "http://smerity.com/media/images/articles/2018/pickpockets.png"
date: 2018-01-08

content: |
    The Turing test is meant to evaluate a machine's ability to exhibit intelligent behaviour equivalent to, or indistinguishable from, that of a human.
    A human evaluator is asked to converse with two conversational agents - one a human, the other a machine - and determine which is the machine.

    Just as many academic tests and datasets aren't representative of the real world however, neither is the Turing test.
    With a simple modification, the Turing test goes from impossible to trivial in real world settings.
    
    In the Turing test, the human evaluator is aware that one of the conversational agents is a machine.
    With this single statement, we've already lost any semblance to reality.
    The Turing test warns the evaluator.
    The real world doesn't give us this luxury.

    **Humanity has already failed the Turing test at a fundamental level.**

    Knowledge that you're being tested ahead of time can be a significant cheat.
    You wouldn't trust a restaurant if they needed advanced notice to pass a health inspection.
    If the manner you act in every day life is entirely different to what it would be under test conditions, such a test is not reflective of your behaviour in real life.

    <!--
    If our conversation based Turing test was replaced with an image based Turing test, this flaw becomes even more obvious.
    In such a Photoshop Turing test, the human evaluator would be shown a real and fake photo with full knowledge one is forged.
    The human can then spend full time and effort analyzing the content.
    In this task, the human stands a far better chance at apprehending the false reality.
    -->

    ## The real world is composed of pickpockets, not magicians

    In the real world, our human evaluator loses two vital defenses.
    First, they're given no indication they're about to be tricked.
    Second, they aren't granted the time or inclination to fully interrogate the interaction.

    <!--
    If our human evaluator is given no indication of the duplicitious nature, all the subtlety necessary to determine real versus fake is lost.
    A glance is likely the best they will dedicate to the task, extracting what they consider the vital information in an instant.
    We lost the context.
    We skip the logic needed to tease out truth.
    What is captured in that instant is the payload - emotional or mental - that is intended to push or pull us into reacting a specific way.

    For the conversational Turing test, we have the same problem if we don't warn the human evaluator.
    When in an online discussion are you allowed to interrogate a concerning poster?
    If you ask a question of them, do you expect a reply?
    Even if you verify they're a machine with a hidden agenda, how many people have already received that payload with no way of clarifying that misinformation?
    Would you even ask yourself whether the person writing the comment may be a machine to begin with?
    -->

    We can compare this to the difference between a magician and a pickpocket.

    When a magician takes the stage, we're already aware they're going to trick us.
    Even heeding such a warning a competent magician can make us believe we've seen the impossible.
    When a magician makes their assistant float in mid air, we don't start questioning the laws of physics, we start questioning our powers of observation.
    Magicians bend reality rather than break it.
    We know this.
    As such, we don't change our opinion or logic regarding gravity upon seeing evidence presented by them.

    A pickpocket uses a subset of the magician's techniques but with a very different aim.
    With relatively minor suggestions they can direct your actions and attention a very specific way, one you'd never intentionally allow given full information.
    In crafting that fog of illusion they can then readily exaggerate and exploit your vulnerabilities.
    That fog may hold you for just an instant, long enough to slip a wallet from your bag, or even for years.
    A snake oil salesmen or fraudster is just a pickpocket with a longer gameplan.

    From this, we can see exactly how the Turing test falls apart in the real world.
    The Turing test, like the magician, aims to entertain, stretching our definition of what's possible whilst returning us to reality at the end.
    Pickpockets, their adversarial counterparts in the real world, exploit your vulnerabilities towards a specific goal, leaving you lost in that fog of illusion.

    <!--
    If they're obvious, they sell snake oil.
    If they're
    -->

    <!--
    A magic trick is when we are told that we are entering a reality where duplicity is apparent.
    Even with such a warning we're left clueless as to how reality has been bent to achieve the given illusion.
    They bend our perception of reality with a specific goal in mind and without any warning beforehand.
    -->

    <!--
    In the Photoshop Turing test, the human may evaluate the images at a glance, capturing none of the subtlety necessary to determine real versus fake.
    In the conversational Turing test, the human may never consider the conversational agent to be duplicitous, and even if they do, may be denied further questions.
    Even if they reveal the duplicitious nature of the agent, they're unable to relay that to the broader community.
    -->

    **Side note:** If you want to see a magician misdirecting and tricking you whilst explaining how this applies to security, [watch the hacker known as "Alex"](https://www.youtube.com/watch?v=nSu11VvVL9M).

    ## The Blind Turing Test

    Can a human evaluator tell the difference between a human and a machine when interacting in a standard real world setting?
    In such a setup the human evaluator may receive zero warning and may be unable to fully interrogate the agent or the duplicitous material they receive.
    Even if a previous evaluator put in the time and effort to reveal the truth, that information may not have been relayed to new participants.

    Disentangling truth from fiction is not a task humans are well equipped for - or one that we even deem generally necessary.
    For much of our life, our defenses are down.
    It doesn't require a sophisticated conversational magic trick to deceive you, it just requires a minor nudge.
    Thus, we're vulnerable to crafted conversations in the right context, just as we are to pickpockets when we enter their domain.
    
    **Note:** Whilst I coined the phrase "blind Turing test" for the setup above, if you know of an existing phrase for this or for similar setups, please tell me :)

    <!--
    In the adversarial setup,
    The threat in the real world is that we are given no priors and these illusions are created and exploited at scale specifically to modify you and your thinking.
    You won't know you're being influenced by it, you won't be cognizant enough to judge the validity of the content, and you will eventually be influenced by those already most vulnerable to that form of persuasion.
    -->

    ## Misinformation is a virus
    <!-- but this virus is engineered -->

    Viral content is aptly named.
    Just as with normal content, misinformation can spread like wildfire on social media.
    We have seen monstrous imagined stories grow in hours.
    Some of these have been from simple misunderstandings whilst others than have been unsophisticated jabs or clickbait.
    <!--From either simple misunderstandings without ill intent or unsophisticated targeting we have seen horrific monsters grow in hours.-->
    In the past year we've been given no lack of examples, ranging from political propaganda to exaggerated news headlines.
    
    As soon as misinformation captures the imagination of the crowd and acquires a groundswell of support, you'll see it morph and evolve.
    This target can be impossible to stop.
    Attempts at fighting back such misinformation is akin to fighters facing down an inferno with a bottle of water.

    **All of this is a result of normal misunderstanding or traditional malice.**

    The way in which we react, share, and possess information as it spreads across social media can already be destructive.
    We adopt positions with little thought or analysis and hold to them with fierce aggression.

    What happens as we continue failing the blind Turing test in the real world?
    What happens when adversarial agents stoke the fire and spread misinformation with a specific underlying purpose?
    Adversarial agents are perfectly suited to build and maintain a fog of war to keep misinformation aloft.

    In computer security, the attack surface is vast and the cost of launching attacks is low.
    Attacks don't need to be sophisticated to catch victims given how large and variable the ecosystem is.
    While some within the community may have worked to be resilient and secure, nothing can be done for the vast majority of those who are vulnerable.

    Misinformation attacks now have the same vast attack surface.
    Exploits can be deployed at scale and crafted specifically for vulnerable humans.
    Misinformation and propaganda can reach far behind enemy lines or through our traditional walls of defence.
    While these adversarial agents may not be sophisticated, the scale at which they're deployed means they will find vulnerable humans.

    As this misinformation spreads, traversing the social graph from bot (adversarial agent) to human, it can become more and more resilient over time.
    Adversarial agents can provide support and social validation to infected humans.
    Infected humans can (even if flawed) apply more complex reasoning and logic to support the misinformation they now believe.

    **At this stage you're no longer arguing against a bot, you're arguing against a human twisted into aligning themselves with a fictitious team or narrative.**

    We as a society still trust by default more than we should in the digital domain.
    We still modify our thinking by trusting sources we shouldn't.
    We place too much signal in soft influence and soft logic relayed by "humans", all nudging you quietly and consistently.
    We still receive trust and authority from the wrong sources and assume there is truth when we hear this at scale.

    **Misinformation is a virus and we've just passed the asymptomatic period of this digitized disease.**

    <!--
    This situation will become worse the more sophisticated these attacks become.
    There's also no specific crime to be charged for.
    -->

    ## There's no solution, only a sober warning

    I have no clean solution for you.
    I have no readily available way to defend yourself.
    All I can say is that if you think you aren't vulnerable to this, you're wrong.
    Everyone in humanity is now a part of this blind Turing test.
    The difficulty of this game is going to increase rapidly.
    Your opponents are armed with your personal information, your preferences, your affiliations, and can deploy millions of calculations each day to tailoring attacks against your psychological vulnerabilities, convincing you to act in their favour.
    Regardless of your politics, regardless of your profession, you should be asking how you're vulnerable and how you could defend yourself.
    A fundamental truth we need to confront is that billions of dollars in both infrastructure and information gathering have been invested in systems that can rapidly be turned against our own interests.

    **Any system or network that has been optimized for advertisements has been implicitly optimized for spreading misinformation.**

    I personally don't think humans are capable of defending themselves against the more sophisticated and targeted attacks that are likely to come.
    I'm not entirely sure how to reinforce the defenses we have or craft the defenses we'll need.

    We're well past the era of mass advertising.
    When launching a campaign, propaganda or product, we will see a shift in targeting.
    They'll no longer be targeting a demographic, they'll be targeting just you.

    *Thanks to [François Chollet](https://twitter.com/fchollet) for early discussions, [Alex Hogue](https://mango.pdf.zone/) for draft feedback, and Ross Edwin Thompson for [the pickpocket graphic](https://www.flickr.com/photos/ross_edwin/24788097798/).*

    <!--
    ## Propaganda and advertising go hand in hand

    If you're reading reviews, how many of those are real?

    The same techniques that we're beginning to see for propaganda have either already or will soon be used for traditional marketing.

    If you're on Reddit and someone mentions a computer game, "My friend loved this game - primarily the plot - but it didn't work as well for me. I did like it though - I just don't think it was a match."

    ## There's no clean solution, only continued questions

    As it traverses the social graph, misinformation becomes more and more resilient over time.
    While the adversarial agents may not be sophisticated, the scale at which they're deployed can convince humans en masse.
    These newly infected humans can make faulty arguments and logic more convincing.
    Reference past discussion or sources as if they're authoritative.
    The faulty arguments can be made more and more convincing.
    Social proof / validation without tied identities. 
    If an infected host fails to spread misinformation to a new community, they can always find solace in returning to their original community.

    Like a virus that is able to protect itself (sphere of virus with anti-bodies only attacking the outside).

    In the adversarial setup, human infected hosts can be reinforced by machines - their opinion supported both in volume and with corroborating misinformation. 

    ## On the internet, no-one knows you're a dog

    In the real world, you as the human evaluator are never told that conversational agents may be machines.
    You as a human evaluator cannot be guaranteed to get a reply from a potentially adversarial conversational agent.

    ## There is no conversational herd immunity

    In computer security, the attack surface is vast and the cost of launching attacks is low.
    Attacks don't need to be sophisticated to catch victims.
    You only need a few attacks to be successful for the adversarial agent to succeed.
    While some within the community may be resilient or secure, nothing can be done for the vast majority of those which are vulnerable.
    Just as a virus, human or computer, infected agents spread from those vulnerable communities to those in secured communities.
    At that stage you're no longer arguing against a bot, you're arguing against a human twisted into aligning themselves with a fictitious team or narrative.

    As opposed to traditional herd immunity, any agent can jump to any location.

    ## Providing community provides a platform for extremists

    ## Language is an API and bots only write, they don't read

    ## Attacking is asymmetric warfare

    The attackers gain energy when those they attack become exasperated.

    ## The erosion of traditional mediation / acclimatization of traditional attack methods

    The misinformation and propaganda can reach far behind enemy lines.
    They can be targeted to individuals without any overhead cost.
    Infected individuals can then assist in infecting others.
    Antibodies.

    ## Outsourcing responsibility for evaluation to third parties - Facebook / Twitter / Google have no incentive to prevent this

    ## We place too much signal in soft influence - a dozen "humans" softly saying X can influence you

    ## This is a virus or infecting transmitters - you have carriers of the disease, immunization, ...
    
    Society is sick - there are symptoms, obvious and hidden, which can indicate an infection.
    The spread of the illness is hidden by the transmitters (social media).
    -->
